Plugins are loaded from /kafka/connect
Using the following environment variables:
      GROUP_ID=1
      CONFIG_STORAGE_TOPIC=my_connect_configs
      OFFSET_STORAGE_TOPIC=my_connect_offsets
      STATUS_STORAGE_TOPIC=my_source_connect_statuses
      BOOTSTRAP_SERVERS=kafka:9092
      REST_HOST_NAME=172.20.0.7
      REST_PORT=8083
      ADVERTISED_HOST_NAME=172.20.0.7
      ADVERTISED_PORT=8083
      KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      OFFSET_FLUSH_INTERVAL_MS=60000
      OFFSET_FLUSH_TIMEOUT_MS=5000
      SHUTDOWN_TIMEOUT=10000
--- Setting property from CONNECT_INTERNAL_VALUE_CONVERTER: internal.value.converter=org.apache.kafka.connect.json.JsonConverter
--- Setting property from CONNECT_VALUE_CONVERTER: value.converter=org.apache.kafka.connect.json.JsonConverter
--- Setting property from CONNECT_REST_ADVERTISED_HOST_NAME: rest.advertised.host.name=172.20.0.7
--- Setting property from CONNECT_OFFSET_FLUSH_INTERVAL_MS: offset.flush.interval.ms=60000
--- Setting property from CONNECT_GROUP_ID: group.id=1
--- Setting property from CONNECT_BOOTSTRAP_SERVERS: bootstrap.servers=kafka:9092
--- Setting property from CONNECT_KEY_CONVERTER: key.converter=org.apache.kafka.connect.json.JsonConverter
--- Setting property from CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS: task.shutdown.graceful.timeout.ms=10000
--- Setting property from CONNECT_REST_HOST_NAME: rest.host.name=172.20.0.7
--- Setting property from CONNECT_PLUGIN_PATH: plugin.path=/kafka/connect
--- Setting property from CONNECT_REST_PORT: rest.port=8083
--- Setting property from CONNECT_OFFSET_FLUSH_TIMEOUT_MS: offset.flush.timeout.ms=5000
--- Setting property from CONNECT_STATUS_STORAGE_TOPIC: status.storage.topic=my_source_connect_statuses
--- Setting property from CONNECT_INTERNAL_KEY_CONVERTER: internal.key.converter=org.apache.kafka.connect.json.JsonConverter
--- Setting property from CONNECT_CONFIG_STORAGE_TOPIC: config.storage.topic=my_connect_configs
--- Setting property from CONNECT_REST_ADVERTISED_PORT: rest.advertised.port=8083
--- Setting property from CONNECT_OFFSET_STORAGE_TOPIC: offset.storage.topic=my_connect_offsets
2023-03-05 12:05:50,454 INFO   ||  WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/kafka/bin/../logs, -Dlog4j.configuration=file:/kafka/config/log4j.properties
	jvm.spec = Oracle Corporation, OpenJDK 64-Bit Server VM, 1.8.0_212, 25.212-b04
	jvm.classpath = /kafka/bin/../libs/activation-1.1.1.jar:/kafka/bin/../libs/aopalliance-repackaged-2.5.0.jar:/kafka/bin/../libs/argparse4j-0.7.0.jar:/kafka/bin/../libs/audience-annotations-0.5.0.jar:/kafka/bin/../libs/avro-1.8.2.jar:/kafka/bin/../libs/common-config-5.1.2.jar:/kafka/bin/../libs/common-utils-5.1.2.jar:/kafka/bin/../libs/commons-lang3-3.8.1.jar:/kafka/bin/../libs/connect-api-2.3.0.jar:/kafka/bin/../libs/connect-basic-auth-extension-2.3.0.jar:/kafka/bin/../libs/connect-file-2.3.0.jar:/kafka/bin/../libs/connect-json-2.3.0.jar:/kafka/bin/../libs/connect-runtime-2.3.0.jar:/kafka/bin/../libs/connect-transforms-2.3.0.jar:/kafka/bin/../libs/guava-20.0.jar:/kafka/bin/../libs/hk2-api-2.5.0.jar:/kafka/bin/../libs/hk2-locator-2.5.0.jar:/kafka/bin/../libs/hk2-utils-2.5.0.jar:/kafka/bin/../libs/jackson-annotations-2.9.9.jar:/kafka/bin/../libs/jackson-core-2.9.9.jar:/kafka/bin/../libs/jackson-core-asl-1.9.13.jar:/kafka/bin/../libs/jackson-databind-2.9.9.jar:/kafka/bin/../libs/jackson-dataformat-csv-2.9.9.jar:/kafka/bin/../libs/jackson-datatype-jdk8-2.9.9.jar:/kafka/bin/../libs/jackson-jaxrs-base-2.9.9.jar:/kafka/bin/../libs/jackson-jaxrs-json-provider-2.9.9.jar:/kafka/bin/../libs/jackson-mapper-asl-1.9.13.jar:/kafka/bin/../libs/jackson-module-jaxb-annotations-2.9.9.jar:/kafka/bin/../libs/jackson-module-paranamer-2.9.9.jar:/kafka/bin/../libs/jackson-module-scala_2.12-2.9.9.jar:/kafka/bin/../libs/jakarta.annotation-api-1.3.4.jar:/kafka/bin/../libs/jakarta.inject-2.5.0.jar:/kafka/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/kafka/bin/../libs/jaxb-api-2.3.0.jar:/kafka/bin/../libs/jersey-client-2.28.jar:/kafka/bin/../libs/jersey-common-2.28.jar:/kafka/bin/../libs/jersey-container-servlet-2.28.jar:/kafka/bin/../libs/jersey-container-servlet-core-2.28.jar:/kafka/bin/../libs/jersey-hk2-2.28.jar:/kafka/bin/../libs/jersey-media-jaxb-2.28.jar:/kafka/bin/../libs/jersey-server-2.28.jar:/kafka/bin/../libs/jetty-client-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-continuation-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-http-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-io-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-security-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-server-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-servlet-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-servlets-9.4.18.v20190429.jar:/kafka/bin/../libs/jetty-util-9.4.18.v20190429.jar:/kafka/bin/../libs/jopt-simple-5.0.4.jar:/kafka/bin/../libs/jsr305-3.0.2.jar:/kafka/bin/../libs/kafka-avro-serializer-5.1.2.jar:/kafka/bin/../libs/kafka-clients-2.3.0.jar:/kafka/bin/../libs/kafka-connect-avro-converter-5.1.2.jar:/kafka/bin/../libs/kafka-log4j-appender-2.3.0.jar:/kafka/bin/../libs/kafka-schema-registry-client-5.1.2.jar:/kafka/bin/../libs/kafka-streams-2.3.0.jar:/kafka/bin/../libs/kafka-streams-examples-2.3.0.jar:/kafka/bin/../libs/kafka-streams-scala_2.12-2.3.0.jar:/kafka/bin/../libs/kafka-streams-test-utils-2.3.0.jar:/kafka/bin/../libs/kafka-tools-2.3.0.jar:/kafka/bin/../libs/kafka_2.12-2.3.0.jar:/kafka/bin/../libs/log4j-1.2.17.jar:/kafka/bin/../libs/lz4-java-1.6.0.jar:/kafka/bin/../libs/maven-artifact-3.6.1.jar:/kafka/bin/../libs/metrics-core-2.2.0.jar:/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/kafka/bin/../libs/paranamer-2.8.jar:/kafka/bin/../libs/plexus-utils-3.2.0.jar:/kafka/bin/../libs/postgresql-42.2.8.jar:/kafka/bin/../libs/reflections-0.9.11.jar:/kafka/bin/../libs/rocksdbjni-5.18.3.jar:/kafka/bin/../libs/scala-library-2.12.8.jar:/kafka/bin/../libs/scala-logging_2.12-3.9.0.jar:/kafka/bin/../libs/scala-reflect-2.12.8.jar:/kafka/bin/../libs/slf4j-api-1.7.26.jar:/kafka/bin/../libs/slf4j-log4j12-1.7.26.jar:/kafka/bin/../libs/snappy-java-1.1.7.3.jar:/kafka/bin/../libs/spotbugs-annotations-3.1.9.jar:/kafka/bin/../libs/validation-api-2.0.1.Final.jar:/kafka/bin/../libs/zkclient-0.11.jar:/kafka/bin/../libs/zookeeper-3.4.14.jar:/kafka/bin/../libs/zstd-jni-1.4.0-1.jar
	os.spec = Linux, amd64, 5.19.0-35-generic
	os.vcpus = 4
   [org.apache.kafka.connect.runtime.WorkerInfo]
2023-03-05 12:05:50,465 INFO   ||  Scanning for plugin classes. This might take a moment ...   [org.apache.kafka.connect.cli.ConnectDistributed]
2023-03-05 12:05:50,519 INFO   ||  Loading plugin from: /kafka/connect/kafka-connect-jdbc   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,832 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/kafka-connect-jdbc/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,832 INFO   ||  Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,837 INFO   ||  Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,837 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,837 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,839 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:50,865 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-sqlserver   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-sqlserver/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Added plugin 'io.debezium.connector.sqlserver.SqlServerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Added plugin 'io.debezium.transforms.ExtractNewRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Added plugin 'io.debezium.transforms.ByLogicalTableRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Added plugin 'io.debezium.transforms.outbox.EventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,245 INFO   ||  Added plugin 'io.debezium.transforms.UnwrapFromEnvelope'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:51,293 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mysql   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,163 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mysql/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,164 INFO   ||  Added plugin 'io.debezium.connector.mysql.MySqlConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,224 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-oracle   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,627 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-oracle/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,627 INFO   ||  Added plugin 'io.debezium.connector.oracle.OracleConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:52,628 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mongodb   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,013 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mongodb/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,013 INFO   ||  Added plugin 'io.debezium.connector.mongodb.MongoDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,013 INFO   ||  Added plugin 'io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelope'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,013 INFO   ||  Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,014 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-postgres   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,464 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-postgres/}   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:53,465 INFO   ||  Added plugin 'io.debezium.connector.postgresql.PostgresConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,333 INFO   ||  Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.MockConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,334 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'io.confluent.connect.avro.AvroConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,335 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,336 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,336 INFO   ||  Added plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,336 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,336 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,337 INFO   ||  Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,337 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,337 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Cast$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ValueToKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Cast$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.RegexRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,338 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,339 INFO   ||  Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,343 INFO   ||  Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,343 INFO   ||  Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,343 INFO   ||  Added aliases 'MongoDbConnector' and 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,343 INFO   ||  Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,344 INFO   ||  Added aliases 'OracleConnector' and 'Oracle' to plugin 'io.debezium.connector.oracle.OracleConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,344 INFO   ||  Added aliases 'PostgresConnector' and 'Postgres' to plugin 'io.debezium.connector.postgresql.PostgresConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,344 INFO   ||  Added aliases 'SqlServerConnector' and 'SqlServer' to plugin 'io.debezium.connector.sqlserver.SqlServerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,344 INFO   ||  Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,344 INFO   ||  Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,345 INFO   ||  Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,346 INFO   ||  Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,346 INFO   ||  Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,346 INFO   ||  Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,346 INFO   ||  Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,346 INFO   ||  Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,347 INFO   ||  Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,347 INFO   ||  Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,347 INFO   ||  Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,347 INFO   ||  Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,348 INFO   ||  Added alias 'UnwrapFromMongoDbEnvelope' to plugin 'io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelope'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,349 INFO   ||  Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,349 INFO   ||  Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,357 INFO   ||  Added alias 'UnwrapFromEnvelope' to plugin 'io.debezium.transforms.UnwrapFromEnvelope'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,358 INFO   ||  Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,361 INFO   ||  Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,361 INFO   ||  Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,362 INFO   ||  Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,362 INFO   ||  Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,362 INFO   ||  Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,362 INFO   ||  Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,362 INFO   ||  Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
2023-03-05 12:05:56,393 INFO   ||  DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = default
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = my_connect_configs
	connect.protocol = compatible
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = 1
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = my_connect_offsets
	plugin.path = [/kafka/connect]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	rest.advertised.host.name = 172.20.0.7
	rest.advertised.listener = null
	rest.advertised.port = 8083
	rest.extension.classes = []
	rest.host.name = 172.20.0.7
	rest.port = 8083
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = my_source_connect_statuses
	task.shutdown.graceful.timeout.ms = 10000
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
   [org.apache.kafka.connect.runtime.distributed.DistributedConfig]
2023-03-05 12:05:56,393 INFO   ||  Worker configuration property 'internal.key.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration.   [org.apache.kafka.connect.runtime.WorkerConfig]
2023-03-05 12:05:56,393 INFO   ||  Worker configuration property 'internal.value.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration.   [org.apache.kafka.connect.runtime.WorkerConfig]
2023-03-05 12:05:56,394 INFO   ||  Creating Kafka admin client   [org.apache.kafka.connect.util.ConnectUtils]
2023-03-05 12:05:56,398 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,547 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,547 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,547 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,547 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,547 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,548 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,549 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:05:56,550 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:05:56,550 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:05:56,550 INFO   ||  Kafka startTimeMs: 1678017956549   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:05:56,611 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:56,715 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:56,816 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:57,119 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:57,522 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:58,327 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:05:59,540 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:02,242 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:03,358 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:05,803 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:06,808 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:07,714 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:08,624 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:10,553 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:11,657 WARN   ||  [AdminClient clientId=adminclient-1] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2023-03-05 12:06:12,842 INFO   ||  Kafka cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.connect.util.ConnectUtils]
2023-03-05 12:06:12,870 INFO   ||  Logging initialized @23111ms to org.eclipse.jetty.util.log.Slf4jLog   [org.eclipse.jetty.util.log]
2023-03-05 12:06:12,934 INFO   ||  Added connector for http://172.20.0.7:8083   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:12,934 INFO   ||  Initializing REST server   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:12,943 INFO   ||  jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_212-b04   [org.eclipse.jetty.server.Server]
2023-03-05 12:06:13,000 INFO   ||  Started http_172.20.0.78083@2488b073{HTTP/1.1,[http/1.1]}{172.20.0.7:8083}   [org.eclipse.jetty.server.AbstractConnector]
2023-03-05 12:06:13,000 INFO   ||  Started @23241ms   [org.eclipse.jetty.server.Server]
2023-03-05 12:06:13,020 INFO   ||  Advertised URI: http://172.20.0.7:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:13,020 INFO   ||  REST server listening at http://172.20.0.7:8083/, advertising URL http://172.20.0.7:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:13,021 INFO   ||  Advertised URI: http://172.20.0.7:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:13,024 INFO   ||  Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden   [org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy]
2023-03-05 12:06:13,029 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,029 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,029 INFO   ||  Kafka startTimeMs: 1678017973029   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,115 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = false
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:13,116 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = false
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:13,145 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,145 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,146 INFO   ||  Kafka startTimeMs: 1678017973145   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,148 INFO   ||  Kafka Connect distributed worker initialization took 22683ms   [org.apache.kafka.connect.cli.ConnectDistributed]
2023-03-05 12:06:13,148 INFO   ||  Kafka Connect starting   [org.apache.kafka.connect.runtime.Connect]
2023-03-05 12:06:13,148 INFO   ||  Initializing REST resources   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:13,148 INFO   ||  [Worker clientId=connect-1, groupId=1] Herder starting   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:13,148 INFO   ||  Worker starting   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:13,149 INFO   ||  Starting KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
2023-03-05 12:06:13,149 INFO   ||  Starting KafkaBasedLog with topic my_connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,149 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,162 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,162 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,163 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,164 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,165 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,165 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,165 INFO   ||  Kafka startTimeMs: 1678017973164   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,211 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,229 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,230 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,230 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,230 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,230 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,230 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,231 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,232 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,232 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,232 INFO   ||  Kafka startTimeMs: 1678017973232   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,242 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,268 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,269 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,269 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,271 INFO   ||  DefaultSessionIdManager workerName=node0   [org.eclipse.jetty.server.session]
2023-03-05 12:06:13,274 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,275 INFO   ||  No SessionScavenger set, using defaults   [org.eclipse.jetty.server.session]
2023-03-05 12:06:13,275 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,275 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,275 INFO   ||  Kafka startTimeMs: 1678017973275   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,276 INFO   ||  node0 Scavenging every 600000ms   [org.eclipse.jetty.server.session]
2023-03-05 12:06:13,340 INFO   ||  [Producer clientId=producer-1] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,364 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Subscribed to partition(s): my_connect_offsets-0, my_connect_offsets-5, my_connect_offsets-10, my_connect_offsets-20, my_connect_offsets-15, my_connect_offsets-9, my_connect_offsets-11, my_connect_offsets-4, my_connect_offsets-16, my_connect_offsets-17, my_connect_offsets-3, my_connect_offsets-24, my_connect_offsets-23, my_connect_offsets-13, my_connect_offsets-18, my_connect_offsets-22, my_connect_offsets-8, my_connect_offsets-2, my_connect_offsets-12, my_connect_offsets-19, my_connect_offsets-14, my_connect_offsets-1, my_connect_offsets-6, my_connect_offsets-7, my_connect_offsets-21   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:06:13,368 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-5   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-10   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-20   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-15   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-9   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-11   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-16   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-17   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-24   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-23   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,369 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-13   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-18   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-22   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-8   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-12   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-19   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-14   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-6   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-7   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,370 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Seeking to EARLIEST offset of partition my_connect_offsets-21   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,389 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,435 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-2 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,436 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-4 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,436 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-6 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,436 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-8 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,436 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,444 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-18 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,444 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-20 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,445 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-22 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,445 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-24 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-10 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-12 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-14 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-16 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-3 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-5 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,447 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-7 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-9 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-1 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-19 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-21 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-23 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-11 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-13 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-15 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,448 INFO   ||  [Consumer clientId=consumer-1, groupId=1] Resetting offset for partition my_connect_offsets-17 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,578 INFO   ||  Finished reading KafkaBasedLog for topic my_connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,581 INFO   ||  Started KafkaBasedLog for topic my_connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,581 INFO   ||  Finished reading offsets topic and starting KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
2023-03-05 12:06:13,582 INFO   ||  Worker started   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:13,582 INFO   ||  Starting KafkaBasedLog with topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,583 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,585 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,586 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,586 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,588 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,588 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,588 INFO   ||  Kafka startTimeMs: 1678017973586   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,601 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,607 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,607 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,608 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,609 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,609 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,609 INFO   ||  Kafka startTimeMs: 1678017973609   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,610 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,640 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,641 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,646 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,647 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,647 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,647 INFO   ||  Kafka startTimeMs: 1678017973647   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,672 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Subscribed to partition(s): my_source_connect_statuses-0, my_source_connect_statuses-4, my_source_connect_statuses-1, my_source_connect_statuses-2, my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:06:13,672 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Seeking to EARLIEST offset of partition my_source_connect_statuses-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,672 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Seeking to EARLIEST offset of partition my_source_connect_statuses-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,672 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Seeking to EARLIEST offset of partition my_source_connect_statuses-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,672 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Seeking to EARLIEST offset of partition my_source_connect_statuses-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,673 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Seeking to EARLIEST offset of partition my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,677 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,686 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Resetting offset for partition my_source_connect_statuses-3 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,687 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Resetting offset for partition my_source_connect_statuses-2 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,687 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Resetting offset for partition my_source_connect_statuses-1 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,687 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Resetting offset for partition my_source_connect_statuses-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,687 INFO   ||  [Consumer clientId=consumer-2, groupId=1] Resetting offset for partition my_source_connect_statuses-4 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,719 INFO   ||  [Producer clientId=producer-2] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,817 INFO   ||  Finished reading KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,817 INFO   ||  Started KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,817 INFO   ||  Starting KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2023-03-05 12:06:13,817 INFO   ||  Starting KafkaBasedLog with topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,818 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,819 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,819 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,819 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,819 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2023-03-05 12:06:13,821 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,821 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,821 INFO   ||  Kafka startTimeMs: 1678017973821   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,849 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'group.id' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,858 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:13,859 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,859 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,860 INFO   ||  Kafka startTimeMs: 1678017973859   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,860 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'rest.advertised.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'task.shutdown.graceful.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'plugin.path' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'status.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'offset.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,863 WARN   ||  The configuration 'config.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'rest.advertised.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'status.storage.topic' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'rest.host.name' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'offset.flush.timeout.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'config.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'offset.flush.interval.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'rest.port' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'internal.key.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'key.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'value.converter.schemas.enable' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'internal.value.converter' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 WARN   ||  The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:13,864 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,864 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,864 INFO   ||  Kafka startTimeMs: 1678017973864   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:13,879 INFO   ||  [Consumer clientId=consumer-3, groupId=1] Subscribed to partition(s): my_connect_configs-0   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:06:13,879 INFO   ||  [Consumer clientId=consumer-3, groupId=1] Seeking to EARLIEST offset of partition my_connect_configs-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,885 INFO   ||  [Consumer clientId=consumer-3, groupId=1] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,902 INFO   ||  [Consumer clientId=consumer-3, groupId=1] Resetting offset for partition my_connect_configs-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:13,911 INFO   ||  Successfully processed removal of connector 'jdbc-sink'   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2023-03-05 12:06:13,912 INFO   ||  Finished reading KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,912 INFO   ||  Started KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2023-03-05 12:06:13,913 INFO   ||  Started KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2023-03-05 12:06:13,913 INFO   ||  [Worker clientId=connect-1, groupId=1] Herder started   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:13,981 INFO   ||  [Worker clientId=connect-1, groupId=1] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:13,983 INFO   ||  [Worker clientId=connect-1, groupId=1] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:13,996 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:06:13,996 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:14,008 INFO   ||  [Producer clientId=producer-3] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:14,043 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:14,045 INFO   ||  Started o.e.j.s.ServletContextHandler@4fbb001b{/,null,AVAILABLE}   [org.eclipse.jetty.server.handler.ContextHandler]
2023-03-05 12:06:14,045 INFO   ||  REST resources initialized; server is started and ready to handle requests   [org.apache.kafka.connect.runtime.rest.RestServer]
2023-03-05 12:06:14,047 INFO   ||  Kafka Connect started   [org.apache.kafka.connect.runtime.Connect]
2023-03-05 12:06:22,904 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 11   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:22,905 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 11 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=11, connectorIds=[jdbc-sink, inventory-connector], taskIds=[jdbc-sink-0, inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:22,905 WARN   ||  [Worker clientId=connect-1, groupId=1] Catching up to assignment's config offset.   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:22,905 INFO   ||  [Worker clientId=connect-1, groupId=1] Current config state offset -1 is behind group assignment 11, reading to end of config log   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:22,997 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished reading to end of log and updated config snapshot, new config log offset: 11   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:22,997 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 11   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:22,998 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:23,000 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connector inventory-connector   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:23,001 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task inventory-connector-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:23,001 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:23,004 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,004 INFO   ||  Creating task inventory-connector-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,005 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:06:23,011 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:06:23,011 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:06:23,005 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:06:23,013 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)
	transforms.route.replacement = $3
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,015 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)
	transforms.route.replacement = $3
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,015 INFO   ||  Creating connector inventory-connector of type io.debezium.connector.mysql.MySqlConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,015 INFO   ||  TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:06:23,016 INFO   ||  Instantiated task inventory-connector-0 with version 0.10.0.Final of type io.debezium.connector.mysql.MySqlConnectorTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,016 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:23,016 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,017 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:23,017 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,017 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,018 INFO   ||  Instantiated connector inventory-connector with version 0.10.0.Final of type class io.debezium.connector.mysql.MySqlConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,020 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,020 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,020 INFO   ||  Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,023 INFO   ||  Instantiated connector jdbc-sink with version 5.3.2 of type class io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,023 INFO   ||  TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:06:23,024 INFO   ||  Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,025 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:23,025 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,025 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:06:23,025 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,025 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,025 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.RegexRouter}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,039 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-inventory-connector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:23,046 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,046 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,047 INFO   ||  Kafka startTimeMs: 1678017983046   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,051 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,055 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:06:23,055 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,056 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:23,078 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,079 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,079 INFO   ||  Kafka startTimeMs: 1678017983078   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,086 INFO   ||  Finished creating connector inventory-connector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,088 INFO   ||  Finished creating connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:06:23,102 INFO   ||  SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	value.converter = null
   [org.apache.kafka.connect.runtime.SourceConnectorConfig]
2023-03-05 12:06:23,102 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:06:23,104 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)
	transforms.route.replacement = $3
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,104 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:06:23,109 INFO   ||  Setting task configurations for 1 workers.   [io.confluent.connect.jdbc.JdbcSinkConnector]
2023-03-05 12:06:23,133 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:06:23,139 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): cr_th_activo   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:06:23,141 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:06:23,150 INFO   ||  Starting MySqlConnectorTask with configuration:   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,154 INFO   ||  [Producer clientId=connector-producer-inventory-connector-0] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:23,154 INFO   ||     connector.class = io.debezium.connector.mysql.MySqlConnector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.user = debezium   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.server.id = 184054   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     tasks.max = 1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.history.kafka.bootstrap.servers = kafka:9092   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.history.kafka.topic = schema-changes.cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     transforms = route   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.server.name = dbserver1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.port = 3306   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     transforms.route.type = org.apache.kafka.connect.transforms.RegexRouter   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     task.class = io.debezium.connector.mysql.MySqlConnectorTask   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.hostname = mysql   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.password = ********   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     name = inventory-connector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     transforms.route.replacement = $3   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,161 INFO   ||     database.include = cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:06:23,184 INFO   ||  JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = null
	connection.url = jdbc:postgresql://postgres:5432/inventoryDB?user=postgres&password=postgres
	connection.user = null
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2023-03-05 12:06:23,187 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception   [org.apache.kafka.connect.runtime.WorkerTask]
org.apache.kafka.common.config.ConfigException: Primary key mode must be 'record_key' when delete support is enabled
	at io.confluent.connect.jdbc.sink.JdbcSinkConfig.<init>(JdbcSinkConfig.java:458)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.start(JdbcSinkTask.java:45)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:300)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:189)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:06:23,190 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted   [org.apache.kafka.connect.runtime.WorkerTask]
2023-03-05 12:06:23,190 INFO   ||  Stopping task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:06:23,190 WARN   ||  Could not stop task   [org.apache.kafka.connect.runtime.WorkerSinkTask]
java.lang.NullPointerException
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.stop(JdbcSinkTask.java:107)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.close(WorkerSinkTask.java:158)
	at org.apache.kafka.connect.runtime.WorkerTask.doClose(WorkerTask.java:156)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:183)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:06:23,483 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=inventory-connector-dbhistory, auto.offset.reset=earliest, session.timeout.ms=10000, bootstrap.servers=kafka:9092, client.id=inventory-connector-dbhistory, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:06:23,483 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Producer config: {bootstrap.servers=kafka:9092, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=inventory-connector-dbhistory, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:06:23,486 INFO   MySQL|dbserver1|task  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [kafka:9092]
	buffer.memory = 1048576
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:06:23,488 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,488 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,488 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678017983488   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,489 INFO   MySQL|dbserver1|task  Found existing offset: {file=mysql-bin.000005, pos=154}   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:06:23,491 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:23,495 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,495 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,495 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678017983495   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,506 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:23,532 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:06:23,536 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,536 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,536 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678017983536   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:06:23,537 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Subscribed to topic(s): schema-changes.cris2   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:06:23,546 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:23,561 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:23,562 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:06:23,568 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:23,576 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:23,592 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:23,595 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Setting newly assigned partitions: schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:06:23,598 INFO   MySQL|dbserver1|task  [Producer clientId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:06:23,613 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Found no committed offset for partition schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:06:23,632 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Resetting offset for partition schema-changes.cris2-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:06:24,570 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Member inventory-connector-dbhistory-34d94df4-016b-4df0-83c7-b56f0f5ee023 sending LeaveGroup request to coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:06:24,586 INFO   MySQL|dbserver1|task  Step 0: Get all known binlogs from MySQL   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:06:24,588 INFO   MySQL|dbserver1|task  MySQL has the binlog file 'mysql-bin.000005' required by the connector   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:06:24,607 INFO   MySQL|dbserver1|task  Requested thread factory for connector MySqlConnector, id = dbserver1 named = binlog-client   [io.debezium.util.Threads]
2023-03-05 12:06:24,619 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:06:24,624 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:06:24,770 INFO   MySQL|dbserver1|binlog  Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000005', pos=154, skipping 0 events plus 0 rows   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:06:24,771 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Source task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:06:24,772 INFO   MySQL|dbserver1|binlog  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:07:23,073 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:07:23,074 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:08:23,075 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:08:23,076 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:09:23,076 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:09:23,077 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:10:23,077 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:10:23,078 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:11:23,079 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:11:23,079 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:12:23,080 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:12:23,081 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:13:23,082 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:13:23,083 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:14:23,084 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:14:23,084 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:15:23,085 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:15:23,086 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:16:23,086 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:16:23,086 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:17:23,087 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:17:23,088 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:18:23,088 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:18:23,089 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:19:23,090 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:19:23,090 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:20:23,090 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:20:23,090 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:21:23,091 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:21:23,091 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:21:31,684 INFO   ||  Successfully processed removal of connector 'jdbc-sink'   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2023-03-05 12:21:31,685 INFO   ||  [Worker clientId=connect-1, groupId=1] Connector jdbc-sink config removed   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,687 INFO   ||  [Worker clientId=connect-1, groupId=1] Handling connector-only config update by stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,687 INFO   ||  Stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:21:31,690 INFO   ||  Stopped connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:21:31,690 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:21:31,690 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:21:31,723 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 12   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:21:31,726 INFO   ||  Stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:21:31,726 WARN   ||  Ignoring stop request for unowned connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:21:31,726 INFO   ||  Stopping task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:21:31,727 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished stopping tasks in preparation for rebalance   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,727 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 12 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=13, connectorIds=[inventory-connector], taskIds=[inventory-connector-0], revokedConnectorIds=[jdbc-sink], revokedTaskIds=[jdbc-sink-0], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,728 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 13   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,728 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,729 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:21:31,729 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:21:31,760 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 13   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:21:31,760 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 13 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=13, connectorIds=[inventory-connector], taskIds=[inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,761 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 13   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:21:31,761 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:22:23,092 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:22:23,092 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:22:59,702 ERROR  ||  Uncaught exception in REST call to /connectors/   [org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper]
java.lang.NullPointerException
	at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.createConnector(ConnectorsResource.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:679)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:392)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:365)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:318)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1700)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1667)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:23:16,496 INFO   ||  AbstractConfig values: 
   [org.apache.kafka.common.config.AbstractConfig]
2023-03-05 12:23:16,522 INFO   ||  [Worker clientId=connect-1, groupId=1] Connector jdbc-sink config updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:16,533 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:23:16,534 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:16,560 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 14   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:16,560 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 14 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=14, connectorIds=[jdbc-sink, inventory-connector], taskIds=[jdbc-sink-0, inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:16,561 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 14   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:16,575 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:16,576 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:23:16,578 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:16,578 INFO   ||  Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,578 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:16,579 INFO   ||  Instantiated connector jdbc-sink with version 5.3.2 of type class io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,600 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,601 INFO   ||  Finished creating connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,602 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:23:16,603 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:16,603 INFO   ||  Setting task configurations for 1 workers.   [io.confluent.connect.jdbc.JdbcSinkConnector]
2023-03-05 12:23:16,613 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:23:16,619 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:16,619 INFO   ||  TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:23:16,620 INFO   ||  Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,620 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:16,620 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,620 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:16,620 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,621 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,622 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:16,622 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:23:16,622 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:16,623 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:23:16,629 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:16,630 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:16,630 INFO   ||  Kafka startTimeMs: 1678018996629   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:16,651 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): cr_th_activo   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:23:16,651 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:23:16,652 INFO   ||  JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = null
	connection.url = jdbc:postgresql://postgres:5432/inventoryDB?user=postgres&password=postgres
	connection.user = null
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2023-03-05 12:23:16,652 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception   [org.apache.kafka.connect.runtime.WorkerTask]
org.apache.kafka.common.config.ConfigException: Primary key mode must be 'record_key' when delete support is enabled
	at io.confluent.connect.jdbc.sink.JdbcSinkConfig.<init>(JdbcSinkConfig.java:458)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.start(JdbcSinkTask.java:45)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:300)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:189)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:23:16,653 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted   [org.apache.kafka.connect.runtime.WorkerTask]
2023-03-05 12:23:16,653 INFO   ||  Stopping task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:23:16,653 WARN   ||  Could not stop task   [org.apache.kafka.connect.runtime.WorkerSinkTask]
java.lang.NullPointerException
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.stop(JdbcSinkTask.java:107)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.close(WorkerSinkTask.java:158)
	at org.apache.kafka.connect.runtime.WorkerTask.doClose(WorkerTask.java:156)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:183)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:23:17,050 INFO   ||  [Worker clientId=connect-1, groupId=1] Tasks [inventory-connector-0, jdbc-sink-0] configs updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,554 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,556 INFO   ||  [Worker clientId=connect-1, groupId=1] Handling task config update by restarting tasks [inventory-connector-0, jdbc-sink-0]   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,557 INFO   ||  Stopping task inventory-connector-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,557 INFO   MySQL|dbserver1|task  Stopping MySQL connector task   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:23:17,558 INFO   MySQL|dbserver1|task  ChainedReader: Stopping the binlog reader   [io.debezium.connector.mysql.ChainedReader]
2023-03-05 12:23:17,558 INFO   MySQL|dbserver1|task  Discarding 0 unsent record(s) due to the connector shutting down   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:23:17,560 INFO   MySQL|dbserver1|task  Discarding 0 unsent record(s) due to the connector shutting down   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:23:17,562 INFO   MySQL|dbserver1|binlog  Stopped reading binlog after 0 events, no new offset was recorded   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:23:17,562 INFO   MySQL|dbserver1|task  [Producer clientId=inventory-connector-dbhistory] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2023-03-05 12:23:17,579 INFO   MySQL|dbserver1|task  Connector task finished all work and is now shutdown   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:23:17,581 INFO   ||  Stopping task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,598 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:23:17,599 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:23:17,600 INFO   ||  [Producer clientId=connector-producer-inventory-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2023-03-05 12:23:17,609 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:23:17,609 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,625 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 15   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,626 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 15 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=16, connectorIds=[jdbc-sink, inventory-connector], taskIds=[jdbc-sink-0, inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,626 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 16   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,626 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,626 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,627 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:23:17,627 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:17,627 INFO   ||  TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:23:17,627 INFO   ||  Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,628 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:17,628 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,628 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:17,628 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,628 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,629 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,629 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:23:17,629 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:17,630 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:23:17,632 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,632 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,632 INFO   ||  Kafka startTimeMs: 1678018997632   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,636 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task inventory-connector-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,637 INFO   ||  Creating task inventory-connector-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,637 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:23:17,637 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)
	transforms.route.replacement = $3
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:23:17,637 INFO   ||  TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:23:17,637 INFO   ||  Instantiated task inventory-connector-0 with version 0.10.0.Final of type io.debezium.connector.mysql.MySqlConnectorTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,638 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:17,638 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,638 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:23:17,638 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,638 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,639 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.RegexRouter}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:23:17,639 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-inventory-connector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:23:17,650 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,650 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,650 INFO   ||  Kafka startTimeMs: 1678018997649   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:17,653 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:23:17,660 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): cr_th_activo   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:23:17,662 INFO   ||  Starting MySqlConnectorTask with configuration:   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||     connector.class = io.debezium.connector.mysql.MySqlConnector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||     database.user = debezium   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||     database.server.id = 184054   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||     tasks.max = 1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||     database.history.kafka.bootstrap.servers = kafka:9092   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,662 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:23:17,666 INFO   ||     database.history.kafka.topic = schema-changes.cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     transforms = route   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     database.server.name = dbserver1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     database.port = 3306   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     transforms.route.type = org.apache.kafka.connect.transforms.RegexRouter   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     task.class = io.debezium.connector.mysql.MySqlConnectorTask   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     database.hostname = mysql   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     database.password = ********   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     name = inventory-connector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     transforms.route.replacement = $3   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,666 INFO   ||     database.include = cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:23:17,665 INFO   ||  JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = null
	connection.url = jdbc:postgresql://postgres:5432/inventoryDB?user=postgres&password=postgres
	connection.user = null
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2023-03-05 12:23:17,693 INFO   ||  Initializing writer using SQL dialect: PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:23:17,695 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2023-03-05 12:23:17,711 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:23:17,711 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,716 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:17,717 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,723 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,735 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:17,735 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting newly assigned partitions: cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:17,740 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:17,754 INFO   ||  [Producer clientId=connector-producer-inventory-connector-0] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:23:17,755 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition cr_th_activo-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:23:17,797 INFO   ||  Attempting to open connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
2023-03-05 12:23:17,866 INFO   ||  JdbcDbWriter Connected   [io.confluent.connect.jdbc.sink.JdbcDbWriter]
2023-03-05 12:23:17,876 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:23:17,905 INFO   ||  Using PostgreSql dialect table "cr_th_activo" absent   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:23:17,908 INFO   ||  Creating table with sql: CREATE TABLE "cr_th_activo" (
"name" TEXT NOT NULL,
"created_at" TEXT DEFAULT '1970-01-01T00:00:00Z',
"mobile_number" TEXT NULL,
"email" TEXT NOT NULL)   [io.confluent.connect.jdbc.sink.DbStructure]
2023-03-05 12:23:17,934 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:23:17,941 INFO   ||  Using PostgreSql dialect table "cr_th_activo" present   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:23:17,966 INFO   ||  Setting metadata for table "cr_th_activo" to Table{name='"cr_th_activo"', columns=[Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'mobile_number', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'name', isPrimaryKey=false, allowsNull=false, sqlType=text}]}   [io.confluent.connect.jdbc.util.TableDefinitions]
2023-03-05 12:23:17,967 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted.   [org.apache.kafka.connect.runtime.WorkerSinkTask]
org.apache.kafka.connect.errors.ConnectException: Write to table '"cr_th_activo"' in UPSERT mode requires key field names to be known, check the primary key configuration
	at io.confluent.connect.jdbc.sink.BufferedRecords.getInsertSql(BufferedRecords.java:267)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:127)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:538)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:321)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:224)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:192)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:23:17,968 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception   [org.apache.kafka.connect.runtime.WorkerTask]
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:560)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:321)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:224)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:192)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Write to table '"cr_th_activo"' in UPSERT mode requires key field names to be known, check the primary key configuration
	at io.confluent.connect.jdbc.sink.BufferedRecords.getInsertSql(BufferedRecords.java:267)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:127)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:538)
	... 10 more
2023-03-05 12:23:17,969 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted   [org.apache.kafka.connect.runtime.WorkerTask]
2023-03-05 12:23:17,969 INFO   ||  Stopping task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:23:17,969 INFO   ||  Closing connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
2023-03-05 12:23:17,970 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-3afb9da8-e400-4de3-bbe4-42a7c4a370d2 sending LeaveGroup request to coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,120 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=inventory-connector-dbhistory, auto.offset.reset=earliest, session.timeout.ms=10000, bootstrap.servers=kafka:9092, client.id=inventory-connector-dbhistory, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:23:18,120 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Producer config: {bootstrap.servers=kafka:9092, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=inventory-connector-dbhistory, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:23:18,121 INFO   MySQL|dbserver1|task  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [kafka:9092]
	buffer.memory = 1048576
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:23:18,123 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,123 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,123 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678018998123   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,124 INFO   MySQL|dbserver1|task  Found existing offset: {file=mysql-bin.000005, pos=154}   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:23:18,127 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:23:18,128 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,128 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,128 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678018998128   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,135 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:23:18,141 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:23:18,145 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,145 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,145 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678018998145   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:23:18,145 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Subscribed to topic(s): schema-changes.cris2   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:23:18,160 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:23:18,166 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,167 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:18,168 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,189 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,209 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,212 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Setting newly assigned partitions: schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:18,215 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Found no committed offset for partition schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:23:18,225 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Resetting offset for partition schema-changes.cris2-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:23:18,236 INFO   MySQL|dbserver1|task  [Producer clientId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:23:18,594 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Member inventory-connector-dbhistory-365c6ffb-726c-485a-b52e-0e57c0c4c308 sending LeaveGroup request to coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:23:18,609 INFO   MySQL|dbserver1|task  Step 0: Get all known binlogs from MySQL   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:23:18,613 INFO   MySQL|dbserver1|task  MySQL has the binlog file 'mysql-bin.000005' required by the connector   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:23:18,636 INFO   MySQL|dbserver1|task  Requested thread factory for connector MySqlConnector, id = dbserver1 named = binlog-client   [io.debezium.util.Threads]
2023-03-05 12:23:18,648 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:23:18,656 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:23:18,791 INFO   MySQL|dbserver1|binlog  Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000005', pos=154, skipping 0 events plus 0 rows   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:23:18,791 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Source task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:23:18,791 INFO   MySQL|dbserver1|binlog  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:24:17,652 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:24:17,653 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:25:17,655 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:25:17,655 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:26:17,656 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:26:17,656 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:27:17,657 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:27:17,657 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:28:17,658 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:28:17,658 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:29:14,411 INFO   ||  Successfully processed removal of connector 'jdbc-sink'   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2023-03-05 12:29:14,411 INFO   ||  [Worker clientId=connect-1, groupId=1] Connector jdbc-sink config removed   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,419 INFO   ||  [Worker clientId=connect-1, groupId=1] Handling connector-only config update by stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,419 INFO   ||  Stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:14,421 INFO   ||  Stopped connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:14,421 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:29:14,422 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:14,438 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 16   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:14,439 INFO   ||  Stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:14,439 INFO   ||  Stopping task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:14,439 WARN   ||  Ignoring stop request for unowned connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:14,440 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished stopping tasks in preparation for rebalance   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,440 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 16 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=18, connectorIds=[inventory-connector], taskIds=[inventory-connector-0], revokedConnectorIds=[jdbc-sink], revokedTaskIds=[jdbc-sink-0], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,441 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 18   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,441 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,441 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:29:14,442 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:14,465 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 17   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:14,465 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 17 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=18, connectorIds=[inventory-connector], taskIds=[inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,466 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 18   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:14,466 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:17,659 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:29:17,659 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:29:26,277 INFO   ||  AbstractConfig values: 
   [org.apache.kafka.common.config.AbstractConfig]
2023-03-05 12:29:26,283 INFO   ||  [Worker clientId=connect-1, groupId=1] Connector jdbc-sink config updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:26,789 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:29:26,789 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,796 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 18   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,797 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 18 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=19, connectorIds=[jdbc-sink, inventory-connector], taskIds=[jdbc-sink-0, inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:26,797 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 19   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:26,797 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:26,797 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:26,797 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,798 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:29:26,798 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:29:26,798 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:26,798 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:26,798 INFO   ||  Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,798 INFO   ||  TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:29:26,800 INFO   ||  Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,800 INFO   ||  Instantiated connector jdbc-sink with version 5.3.2 of type class io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,800 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:26,800 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,800 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:26,800 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,801 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,801 INFO   ||  Finished creating connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,802 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:29:26,802 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:26,802 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:29:26,803 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:26,803 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:29:26,805 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:26,811 INFO   ||  Setting task configurations for 1 workers.   [io.confluent.connect.jdbc.JdbcSinkConnector]
2023-03-05 12:29:26,827 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:26,828 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:26,828 INFO   ||  Kafka startTimeMs: 1678019366827   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:26,842 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): cr_th_activo   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:29:26,844 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:29:26,845 INFO   ||  JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = null
	connection.url = jdbc:postgresql://postgres:5432/inventoryDB?user=postgres&password=postgres
	connection.user = null
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2023-03-05 12:29:26,845 INFO   ||  Initializing writer using SQL dialect: PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:29:26,845 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2023-03-05 12:29:26,870 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:26,870 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,871 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:26,871 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,873 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,877 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:26,879 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting newly assigned partitions: cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:26,880 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:26,883 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition cr_th_activo-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:29:26,888 INFO   ||  Attempting to open connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
2023-03-05 12:29:26,895 INFO   ||  JdbcDbWriter Connected   [io.confluent.connect.jdbc.sink.JdbcDbWriter]
2023-03-05 12:29:26,899 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:26,903 INFO   ||  Using PostgreSql dialect table "cr_th_activo" absent   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:26,903 INFO   ||  Creating table with sql: CREATE TABLE "cr_th_activo" (
"name" TEXT NOT NULL,
"created_at" TEXT DEFAULT '1970-01-01T00:00:00Z',
"mobile_number" TEXT NULL,
"email" TEXT NOT NULL)   [io.confluent.connect.jdbc.sink.DbStructure]
2023-03-05 12:29:26,912 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:26,923 INFO   ||  Using PostgreSql dialect table "cr_th_activo" present   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:26,940 INFO   ||  Setting metadata for table "cr_th_activo" to Table{name='"cr_th_activo"', columns=[Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'mobile_number', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'name', isPrimaryKey=false, allowsNull=false, sqlType=text}]}   [io.confluent.connect.jdbc.util.TableDefinitions]
2023-03-05 12:29:26,942 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted.   [org.apache.kafka.connect.runtime.WorkerSinkTask]
org.apache.kafka.connect.errors.ConnectException: Write to table '"cr_th_activo"' in UPSERT mode requires key field names to be known, check the primary key configuration
	at io.confluent.connect.jdbc.sink.BufferedRecords.getInsertSql(BufferedRecords.java:267)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:127)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:538)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:321)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:224)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:192)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-03-05 12:29:26,944 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception   [org.apache.kafka.connect.runtime.WorkerTask]
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:560)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:321)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:224)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:192)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Write to table '"cr_th_activo"' in UPSERT mode requires key field names to be known, check the primary key configuration
	at io.confluent.connect.jdbc.sink.BufferedRecords.getInsertSql(BufferedRecords.java:267)
	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:127)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:538)
	... 10 more
2023-03-05 12:29:26,944 ERROR  ||  WorkerSinkTask{id=jdbc-sink-0} Task is being killed and will not recover until manually restarted   [org.apache.kafka.connect.runtime.WorkerTask]
2023-03-05 12:29:26,944 INFO   ||  Stopping task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:29:26,944 INFO   ||  Closing connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
2023-03-05 12:29:26,947 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-662318b4-2084-4ac6-a065-448e45332104 sending LeaveGroup request to coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,297 INFO   ||  [Worker clientId=connect-1, groupId=1] Tasks [inventory-connector-0, jdbc-sink-0] configs updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,297 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,297 INFO   ||  [Worker clientId=connect-1, groupId=1] Handling task config update by restarting tasks [inventory-connector-0, jdbc-sink-0]   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,297 INFO   ||  Stopping task inventory-connector-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,298 INFO   MySQL|dbserver1|task  Stopping MySQL connector task   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:29:27,298 INFO   MySQL|dbserver1|task  ChainedReader: Stopping the binlog reader   [io.debezium.connector.mysql.ChainedReader]
2023-03-05 12:29:27,298 INFO   MySQL|dbserver1|task  Discarding 0 unsent record(s) due to the connector shutting down   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:29:27,299 INFO   MySQL|dbserver1|binlog  Stopped reading binlog after 0 events, no new offset was recorded   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:29:27,299 INFO   MySQL|dbserver1|task  Discarding 0 unsent record(s) due to the connector shutting down   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:29:27,299 INFO   MySQL|dbserver1|task  [Producer clientId=inventory-connector-dbhistory] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2023-03-05 12:29:27,301 INFO   MySQL|dbserver1|task  Connector task finished all work and is now shutdown   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:29:27,301 INFO   ||  Stopping task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,636 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:29:27,637 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:29:27,637 INFO   ||  [Producer clientId=connector-producer-inventory-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2023-03-05 12:29:27,640 INFO   ||  [Worker clientId=connect-1, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2023-03-05 12:29:27,640 INFO   ||  [Worker clientId=connect-1, groupId=1] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,645 INFO   ||  [Worker clientId=connect-1, groupId=1] Successfully joined group with generation 19   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,645 INFO   ||  [Worker clientId=connect-1, groupId=1] Joined group at generation 19 and got assignment: Assignment{error=0, leader='connect-1-7cdb4549-4dd7-4b1a-9e32-d16538cd68dd', leaderUrl='http://172.20.0.7:8083/', offset=21, connectorIds=[jdbc-sink, inventory-connector], taskIds=[jdbc-sink-0, inventory-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,645 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting connectors and tasks using config offset 21   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,646 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,646 INFO   ||  [Worker clientId=connect-1, groupId=1] Starting task inventory-connector-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,646 INFO   ||  Creating task inventory-connector-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,646 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,646 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:29:27,646 INFO   ||  ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2023-03-05 12:29:27,647 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:27,647 INFO   ||  TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:29:27,647 INFO   ||  Instantiated task jdbc-sink-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,647 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = inventory-connector
	tasks.max = 1
	transforms = [route]
	transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)
	transforms.route.replacement = $3
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:27,648 INFO   ||  TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2023-03-05 12:29:27,648 INFO   ||  Instantiated task inventory-connector-0 with version 0.10.0.Final of type io.debezium.connector.mysql.MySqlConnectorTask   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,648 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:27,649 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,649 INFO   ||  JsonConverterConfig values: 
	converter.type = key
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:27,649 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:27,649 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,649 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,649 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,650 INFO   ||  JsonConverterConfig values: 
	converter.type = value
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2023-03-05 12:29:27,650 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,650 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task inventory-connector-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,650 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.RegexRouter}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,650 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState}   [org.apache.kafka.connect.runtime.Worker]
2023-03-05 12:29:27,650 INFO   ||  SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2023-03-05 12:29:27,651 INFO   ||  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-inventory-connector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:29:27,651 INFO   ||  EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	tasks.max = 1
	topics = [cr_th_activo]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2023-03-05 12:29:27,652 INFO   ||  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:29:27,654 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,656 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,656 INFO   ||  Kafka startTimeMs: 1678019367654   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,657 INFO   ||  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,657 INFO   ||  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,657 INFO   ||  Kafka startTimeMs: 1678019367657   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,658 INFO   ||  [Worker clientId=connect-1, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2023-03-05 12:29:27,660 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): cr_th_activo   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:29:27,660 INFO   ||  Starting MySqlConnectorTask with configuration:   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,660 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:29:27,662 INFO   ||     connector.class = io.debezium.connector.mysql.MySqlConnector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.user = debezium   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.server.id = 184054   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     tasks.max = 1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.history.kafka.bootstrap.servers = kafka:9092   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.history.kafka.topic = schema-changes.cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     transforms = route   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.server.name = dbserver1   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     database.port = 3306   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     transforms.route.type = org.apache.kafka.connect.transforms.RegexRouter   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||     transforms.route.regex = ([^.]+)\.([^.]+)\.([^.]+)   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,662 INFO   ||  JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = null
	connection.url = jdbc:postgresql://postgres:5432/inventoryDB?user=postgres&password=postgres
	connection.user = null
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2023-03-05 12:29:27,662 INFO   ||     task.class = io.debezium.connector.mysql.MySqlConnectorTask   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||     database.hostname = mysql   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||     database.password = ********   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||     name = inventory-connector   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||     transforms.route.replacement = $3   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||     database.include = cris2   [io.debezium.connector.common.BaseSourceTask]
2023-03-05 12:29:27,663 INFO   ||  Initializing writer using SQL dialect: PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2023-03-05 12:29:27,667 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2023-03-05 12:29:27,672 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:27,672 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,674 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,674 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,679 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,684 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,684 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting newly assigned partitions: cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,685 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition cr_th_activo-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,688 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition cr_th_activo-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:29:27,696 INFO   ||  Attempting to open connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
2023-03-05 12:29:27,725 INFO   ||  JdbcDbWriter Connected   [io.confluent.connect.jdbc.sink.JdbcDbWriter]
2023-03-05 12:29:27,732 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:27,736 INFO   ||  Using PostgreSql dialect table "cr_th_activo" absent   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:27,737 INFO   ||  Creating table with sql: CREATE TABLE "cr_th_activo" (
"name" TEXT NOT NULL,
"created_at" TEXT DEFAULT '1970-01-01T00:00:00Z',
"mobile_number" TEXT NULL,
"email" TEXT NOT NULL)   [io.confluent.connect.jdbc.sink.DbStructure]
2023-03-05 12:29:27,740 INFO   ||  Checking PostgreSql dialect for existence of table "cr_th_activo"   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:27,742 INFO   ||  Using PostgreSql dialect table "cr_th_activo" present   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2023-03-05 12:29:27,746 INFO   ||  Setting metadata for table "cr_th_activo" to Table{name='"cr_th_activo"', columns=[Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'mobile_number', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=text}, Column{'name', isPrimaryKey=false, allowsNull=false, sqlType=text}]}   [io.confluent.connect.jdbc.util.TableDefinitions]
2023-03-05 12:29:27,770 INFO   ||  [Producer clientId=connector-producer-inventory-connector-0] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:27,898 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=inventory-connector-dbhistory, auto.offset.reset=earliest, session.timeout.ms=10000, bootstrap.servers=kafka:9092, client.id=inventory-connector-dbhistory, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:29:27,899 INFO   MySQL|dbserver1|task  KafkaDatabaseHistory Producer config: {bootstrap.servers=kafka:9092, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=inventory-connector-dbhistory, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2023-03-05 12:29:27,899 INFO   MySQL|dbserver1|task  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [kafka:9092]
	buffer.memory = 1048576
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2023-03-05 12:29:27,901 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,901 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,901 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678019367901   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,901 INFO   MySQL|dbserver1|task  Found existing offset: {file=mysql-bin.000005, pos=154}   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:29:27,904 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:29:27,906 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,906 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,906 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678019367906   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,911 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:27,917 INFO   MySQL|dbserver1|task  ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = inventory-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = inventory-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2023-03-05 12:29:27,918 INFO   MySQL|dbserver1|task  Kafka version: 2.3.0   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,918 INFO   MySQL|dbserver1|task  Kafka commitId: fc1aaa116b661c8a   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,918 INFO   MySQL|dbserver1|task  Kafka startTimeMs: 1678019367918   [org.apache.kafka.common.utils.AppInfoParser]
2023-03-05 12:29:27,919 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Subscribed to topic(s): schema-changes.cris2   [org.apache.kafka.clients.consumer.KafkaConsumer]
2023-03-05 12:29:27,921 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:27,924 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Discovered group coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,924 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,924 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,926 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,932 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:27,932 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Setting newly assigned partitions: schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,934 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Found no committed offset for partition schema-changes.cris2-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2023-03-05 12:29:27,941 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Resetting offset for partition schema-changes.cris2-0 to offset 0.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2023-03-05 12:29:28,009 INFO   MySQL|dbserver1|task  [Producer clientId=inventory-connector-dbhistory] Cluster ID: MmUQ7qyLQyqG8lDfQUHkeg   [org.apache.kafka.clients.Metadata]
2023-03-05 12:29:28,120 INFO   MySQL|dbserver1|task  [Consumer clientId=inventory-connector-dbhistory, groupId=inventory-connector-dbhistory] Member inventory-connector-dbhistory-884ab5ef-e63d-45c4-9efb-c28e4ead7b4d sending LeaveGroup request to coordinator 172.20.0.5:9092 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2023-03-05 12:29:28,125 INFO   MySQL|dbserver1|task  Step 0: Get all known binlogs from MySQL   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:29:28,126 INFO   MySQL|dbserver1|task  MySQL has the binlog file 'mysql-bin.000005' required by the connector   [io.debezium.connector.mysql.MySqlConnectorTask]
2023-03-05 12:29:28,130 INFO   MySQL|dbserver1|task  Requested thread factory for connector MySqlConnector, id = dbserver1 named = binlog-client   [io.debezium.util.Threads]
2023-03-05 12:29:28,135 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:29:28,142 INFO   MySQL|dbserver1|task  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:29:28,275 INFO   MySQL|dbserver1|binlog  Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000005', pos=154, skipping 0 events plus 0 rows   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:29:28,275 INFO   MySQL|dbserver1|binlog  Creating thread debezium-mysqlconnector-dbserver1-binlog-client   [io.debezium.util.Threads]
2023-03-05 12:29:28,275 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Source task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:30:27,658 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:30:27,659 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Committing offsets asynchronously using sequence number 1: {cr_th_activo-0=OffsetAndMetadata{offset=8, leaderEpoch=null, metadata=''}}   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2023-03-05 12:30:27,659 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:31:27,659 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:31:27,660 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:32:27,661 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:32:27,661 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:33:27,662 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:33:27,662 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:34:27,663 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:34:27,663 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:35:27,664 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:35:27,665 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:36:27,666 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:36:27,666 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:37:27,667 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:37:27,667 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:38:27,668 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:38:27,668 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:39:27,669 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:39:27,669 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:40:27,670 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:40:27,670 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:41:27,671 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:41:27,671 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:42:27,672 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:42:27,672 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:43:27,673 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:43:27,673 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:44:10,146 INFO   MySQL|dbserver1|binlog  1 records sent during previous 00:14:41.996, last recorded offset: {ts_sec=1678020250, file=mysql-bin.000006, pos=219, row=1, server_id=223344, event=2}   [io.debezium.connector.mysql.BinlogReader]
2023-03-05 12:44:27,673 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:44:27,674 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:44:27,678 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Committing offsets asynchronously using sequence number 15: {cr_th_activo-0=OffsetAndMetadata{offset=9, leaderEpoch=null, metadata=''}}   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2023-03-05 12:44:27,701 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Finished commitOffsets successfully in 27 ms   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:45:27,702 INFO   ||  WorkerSourceTask{id=inventory-connector-0} Committing offsets   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2023-03-05 12:45:27,702 INFO   ||  WorkerSourceTask{id=inventory-connector-0} flushing 0 outstanding messages for offset commit   [org.apache.kafka.connect.runtime.WorkerSourceTask]
